# Topics

---
## 1. Getting Started with Reinforcement Learning
### 1.1 What is RL?
### 1.2 Agent, Environment, and Reward
### 1.3 State, Action, and Policy

---

## 2. Markov Decision Processes (MDPs)
### 2.1 MDP Fundamentals
### 2.2 Return, Discounting, and Value Functions
### 2.3 Bellman Equations and Optimality

---

## 3. Dynamic Programming
### 3.1 Policy Evaluation
### 3.2 Policy Improvement
### 3.3 Value Iteration and Policy Iteration

---

## 4. Monte Carlo Methods
### 4.1 MC Prediction (First-Visit and Every-Visit)
### 4.2 MC Control with Exploring Starts
### 4.3 ε-Greedy and Off-policy Methods

---

## 5. Temporal Difference Learning
### 5.1 TD(0) Prediction
### 5.2 SARSA (On-policy TD Control)
### 5.3 Q-Learning (Off-policy TD Control)

---

## 6. Exploration Strategies
### 6.1 ε-Greedy and Decay Schedules
### 6.2 Softmax Exploration
### 6.3 Upper Confidence Bound (UCB)

---

## 7. Function Approximation
### 7.1 Linear Function Approximation
### 7.2 Using Neural Networks for Value Function Approximation
### 7.3 Stability and Divergence Issues

---

## 8. Deep Reinforcement Learning
### 8.1 Deep Q-Networks (DQN)
### 8.2 Policy Gradient Methods
### 8.3 Actor-Critic Architectures

---

## 9. Projects and Environments
### 9.1 Grid World and Frozen Lake
### 9.2 Cliff Walking and Blackjack
### 9.3 Custom Mini-Games or Simulations

---

## 10. Tools, Visualisations, and Utilities
### 10.1 Logging Episode Rewards
### 10.2 Plotting Policies and Value Maps
### 10.3 Debugging RL Agents

---
